{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use GCP to evaluate the performance of a RAG system using a vertexai index and Gemini.\n",
    "\n",
    "This notebook does not cover how GCP work and will not go over the steps to create the vertexai index (we will however show how to get our embeddings). For more details, you can check [this resource](https://thenewstack.io/how-to-store-embeddings-in-vector-search-and-implement-rag/) or the GCP documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import storage\n",
    "import json\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from openai import AzureOpenAI\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, Tool\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "from vertexai.preview import rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = os.environ.get(\"PROJECT_ID\")\n",
    "BUCKET_NAME = os.environ.get(\"BUCKET_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions to generate our embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_embeddings(sentences, embedding_model = \"text-embedding-004\", location = \"us-central1\"):\n",
    "    \"\"\"\n",
    "    Generates a list of embedding from a list of texts to embed and the name of the embedding model to use.\n",
    "    \n",
    "    input:\n",
    "        sentences (list)\n",
    "        embedding_model (str)\n",
    "\n",
    "    output:\n",
    "        vectors (list)\n",
    "    \"\"\"\n",
    "    aiplatform.init(project=PROJECT_ID, location=location)\n",
    "    model = TextEmbeddingModel.from_pretrained(embedding_model)\n",
    "    embeddings = model.get_embeddings(sentences)\n",
    "    vectors = [embedding.values for embedding in embeddings]\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_embeddings(document_list, sentence_file_path, embed_file_path, batch_size = 100):\n",
    "    \"\"\"\n",
    "    Takes a list of sentences to embed, embeds them and saves the text chunks and embeddings to the specified directories.\n",
    "    \n",
    "    input:\n",
    "        document_list (list)\n",
    "        sentence_file_path (str)\n",
    "        embed_file_path (str)\n",
    "        batch_size (int)\n",
    "    \n",
    "    output:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # GCP embedding has a maximum number of texts you can send at once, that is why we send them in batches.\n",
    "    embeddings = []\n",
    "    for i in range(len(document_list) // batch_size +1):\n",
    "        embeddings += generate_text_embeddings(document_list[i*batch_size:(i+1)*batch_size])\n",
    "    with open(embed_file_path, 'w') as embed_file, open(sentence_file_path, 'w') as sentence_file:\n",
    "        for sentence, embedding in zip(document_list, embeddings):\n",
    "            id = hash(sentence)\n",
    "            \n",
    "            embed_item = {\"id\": id, \"embedding\": embedding}\n",
    "            sentence_item = {\"id\": id, \"sentence\": sentence}\n",
    "            \n",
    "            json.dump(sentence_item, sentence_file)\n",
    "            sentence_file.write('\\n')\n",
    "            json.dump(embed_item, embed_file)\n",
    "            embed_file.write('\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file(bucket_name, file_path, location = \"us-central1\"):\n",
    "    \"\"\"\n",
    "    Uploads a file to a GCP bucket.\n",
    "\n",
    "    input:\n",
    "        bucket_name (str)\n",
    "        file_path (str)\n",
    "        location (str)\n",
    "    \n",
    "    output:\n",
    "        None\n",
    "    \"\"\"\n",
    "    storage_client = storage.Client(project=PROJECT_ID)\n",
    "    # in this function, we create a new bucket. We can adapt the function if we want to use an existing GCP bucket.\n",
    "    #bucket = storage_client.get_bucket(bucket_name)\n",
    "    bucket = storage_client.create_bucket(bucket_name,location=location)\n",
    "    blob = bucket.blob(file_path)\n",
    "    blob.upload_from_filename(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(sentence_file_path):\n",
    "    \"\"\"\n",
    "    Loads a json data containing a json on each line\n",
    "\n",
    "    input:\n",
    "        sentence_file_path (str)\n",
    "\n",
    "    output:\n",
    "        data (list)\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(sentence_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            entry = json.loads(line)\n",
    "            data.append(entry)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_corresponding_document(list_index, sentence_file_list):\n",
    "    \"\"\"\n",
    "    Given a list of index to look for, a list of json with indexes and the corresponding text chunks, returns the text chunks.\n",
    "\n",
    "    input:\n",
    "        list_index (list)\n",
    "        sentence_file_list (list)\n",
    "    output:\n",
    "        document_context (list)\n",
    "    \"\"\"\n",
    "    document_context = []\n",
    "    i=0\n",
    "    for element in sentence_file_list:\n",
    "        if element[\"id\"] in list_index:\n",
    "            i+=1\n",
    "            document_context.append(element[\"sentence\"])\n",
    "        if i == len(list_index):\n",
    "            return document_context\n",
    "    return document_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking of our documents, we upload them online. GCP has no built-in options for chunking. We use langchain to do our chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We retrieve our documents\n",
    "\n",
    "list_doc = []\n",
    "for document in os.listdir(\"sample_data\"):\n",
    "    with open(f\"sample_data/{document}\", encoding='utf-8') as f:\n",
    "        list_doc.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do our chunking using langchain\n",
    "\n",
    "langchain_docs = [\n",
    "    LangchainDocument(page_content=doc)\n",
    "    for doc in tqdm(list_doc)\n",
    "]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=100,\n",
    "    add_start_index=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "docs_processed = []\n",
    "for doc in langchain_docs:\n",
    "    docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "chunk_list = [docs_processed[i].page_content for i in range(len(docs_processed))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_and_save_embeddings(chunk_list, sentence_file_path=\"sentence_file.json\", embed_file_path=\"embed_file.json\")\n",
    "upload_file(BUCKET_NAME, \"embed_file.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interrogation of the documents in our RAG\n",
    "---\n",
    "\n",
    "**Important**: In the meantime, we created in GCP a vector index in vertexai, we deployed this index and created an endpoint for this index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to GCP\n",
    "aiplatform.init(project=PROJECT_ID,location=\"us-central1\")\n",
    "vertexai.init()\n",
    "rag_model = GenerativeModel(\"gemini-1.5-flash-001\")\n",
    "index_test_rag = aiplatform.MatchingEngineIndexEndpoint(index_endpoint_name=\"<Enter Index of your Index Endpoint>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of text chunks.\n",
    "sentence_file_list = load_file(\"sentence_file.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_PROMPT = \"\"\"\n",
    "Your task is to answer a complicated factoid question given documents at your disposal.\n",
    "The questions demand a good understanding of the documents.\n",
    "Your factoid answer should be specific, concise informations retrieved from the documents.\n",
    "\n",
    "Documents:::\n",
    "{document}\n",
    "\n",
    "Question:::\n",
    "{question}\n",
    "\n",
    "Answer:::\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change name of file depending on the name you chose for your question dataset\n",
    "question_dataset = pd.read_csv(\"comparison.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask questions to our LLM\n",
    "\n",
    "list_gemini_answer = []\n",
    "\n",
    "for my_question in question_dataset[\"question\"].values:\n",
    "    question_embedding = generate_text_embeddings([my_question])\n",
    "    response = index_test_rag.find_neighbors(\n",
    "        deployed_index_id = \"<Enter Deployed Index Name here>\",\n",
    "        queries = [question_embedding[0]],\n",
    "        num_neighbors = 5\n",
    "    )\n",
    "    index_to_search = [int(element.id) for element in response[0]]\n",
    "    document_context = find_all_corresponding_document(index_to_search)\n",
    "    document_str = \"\"\n",
    "    for i in range(len(document_context)):\n",
    "        document_str += f\"document number {i}: {document_context[i]}\\n\"\n",
    "\n",
    "    llm_response = rag_model.generate_content(\n",
    "        GEMINI_PROMPT.format(document=document_str, question=my_question)\n",
    "    )\n",
    "    list_gemini_answer.append(llm_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation des réponses\n",
    "\n",
    "---\n",
    "\n",
    "On utilise un modèle GPT-4 pour évaluer les réponses produites par Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_ENDPOINT = os.environ.get(\"OPENAI_API_ENDPOINT\") \n",
    "OPENAI_API_VERSION = os.environ.get(\"OPENAI_API_VERSION\")\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "aoai_client = AzureOpenAI(\n",
    "    azure_endpoint=OPENAI_API_ENDPOINT,\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    api_version=OPENAI_API_VERSION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "\n",
    "###The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evaluate:\n",
    "{response}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "[Is the response correct, accurate, and factual based on the reference answer?]\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "Score 4: The response is mostly correct, accurate, and factual.\n",
    "Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_questions = question_dataset[\"question\"].values\n",
    "all_answer = question_dataset[\"answer\"].values\n",
    "evaluation_gemini = []\n",
    "\n",
    "for i in tqdm(range(len(all_questions))):\n",
    "    question_i = all_questions[i]\n",
    "    reference_answer_i = all_answer[i]\n",
    "    gemini_answer_i = list_gemini_answer[i]\n",
    "    evaluation_gemini.append(aoai_client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": EVALUATION_PROMPT.format(instruction = question_i, response = gemini_answer_i, reference_answer = reference_answer_i)}],\n",
    "        temperature=0.2,\n",
    "        top_p = 1,\n",
    "        max_tokens=800).choices[0].message.content)\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_critique = []\n",
    "gemini_grade = []\n",
    "\n",
    "for i in tqdm(range(len(evaluation_gemini))):\n",
    "    gemini_critique.append(evaluation_gemini[i].split(\"[RESULT]\")[0])\n",
    "    gemini_grade.append(evaluation_gemini[i].split(\"[RESULT]\")[1])\n",
    "\n",
    "question_dataset[\"gemini_anwer\"], question_dataset[\"gemini_critique\"], question_dataset[\"gemini_grade\"] = list_gemini_answer, gemini_critique, gemini_grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_dataset.to_csv(\"comparison.csv\", index=False, sep=\";\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
