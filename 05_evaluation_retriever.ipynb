{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we try to do a more fine-grained evaluation: we will evaluate the retriever to see if we always retrieve the right documents given a user question. In order to do, we will ask the judge model to evaluate if the answer is present in the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries & retrieve questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from openai import AzureOpenAI\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests.auth import AuthBase\n",
    "import time\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_ENDPOINT = os.environ.get(\"OPENAI_API_ENDPOINT\") \n",
    "OPENAI_API_VERSION = os.environ.get(\"OPENAI_API_VERSION\")\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "OPENAI_EMBEDDING_MODEL = os.environ.get(\"OPENAI_EMBEDDING_MODEL\")\n",
    "\n",
    "SEARCH_ENDPOINT = os.environ.get(\"SEARCH_ENDPOINT\")\n",
    "SEARCH_KEY =  os.environ.get(\"SEARCH_KEY\")\n",
    "SEARCH_INDEX = os.environ.get(\"SEARCH_INDEX\")\n",
    "\n",
    "MISTRAL_URL = os.environ.get(\"MISTRAL_URL\")\n",
    "MISTRAL_KEY = os.environ.get(\"MISTRAL_KEY\")\n",
    "\n",
    "aoai_client = AzureOpenAI(\n",
    "    azure_endpoint=OPENAI_API_ENDPOINT,\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    api_version=OPENAI_API_VERSION,\n",
    ")\n",
    "\n",
    "search_client = SearchClient(SEARCH_ENDPOINT, SEARCH_INDEX, AzureKeyCredential(SEARCH_KEY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_dataframe = pd.read_csv(\"comparison.csv\", sep=\";\")\n",
    "reduced_dataset = question_dataframe[[\"question\", \"answer\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever evaluation\n",
    "---\n",
    "\n",
    "Now that we have the questions and answers, we will evaluate the capacity of the retriever of finding chunks of text that can evaluate the answer. In order to do so, we qill ask a powerful AI (GPT-4 in our case), to check if the given answer is in the retrieved chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "Your task is to assess if a given documentation gives you enough information to answer a question.\n",
    "You will be given a documentation, a factoid question and its answer. If the documentation contains the specific, factual piece of information needed to answer the question answer \"True\", else, answer \"False\".\n",
    "Your answers should only be \"True\" or \"False\".\n",
    "\n",
    "###Documentation to evaluate:\n",
    "{documentation}\n",
    "\n",
    "###Question and Answer to look for:\n",
    "question: {question}\n",
    "response: {response}\n",
    "\n",
    "###Your answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to embed a question using Azure OpenAI so that we can do hybrid search in Azure AI Search\n",
    "def generate_embeddings(user_question, aoai_client=aoai_client):\n",
    "    return aoai_client.embeddings.create(input = [user_question], model=OPENAI_EMBEDDING_MODEL).data[0].embedding\n",
    "\n",
    "def retrieve_doc(user_question, nb_doc=5, search_client=search_client):\n",
    "        \"\"\"\n",
    "        Function that takes as input a user question and uses Azure AI Search to return a list of documents and url that are on the topic of the user question.\n",
    "\n",
    "        input :\n",
    "            user_intent (str)\n",
    "\n",
    "        output :\n",
    "            doc_list (list)\n",
    "            url_list (list)\n",
    "        \"\"\"\n",
    "        embedded_query = generate_embeddings(user_question)\n",
    "        vector_query = VectorizedQuery(vector=embedded_query, k_nearest_neighbors=nb_doc, fields=\"contentVector\")\n",
    "        ai_search_results = search_client.search(\n",
    "            search_text = user_question,\n",
    "            search_fields=[\"content\"],\n",
    "            vector_queries = [vector_query],\n",
    "            select = [\"content\", \"title\", \"url\"],\n",
    "            query_type = \"semantic\",\n",
    "            semantic_configuration_name=\"default\",\n",
    "            top=nb_doc\n",
    "        )\n",
    "        \n",
    "        # Important documents and list of urls associated to the documents\n",
    "        url_list = []\n",
    "        doc_list = []\n",
    "        for my_result in ai_search_results:\n",
    "            doc_list.append(my_result[\"content\"])\n",
    "            url_list.append(my_result[\"url\"])\n",
    "        return list(set(doc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_evaluation = []\n",
    "\n",
    "for my_question_answer in reduced_dataset.values:\n",
    "    list_doc = retrieve_doc(my_question_answer[0])\n",
    "    document_str = \"\"\n",
    "    for i in range(len(list_doc)):\n",
    "        document_str += f\"document number {i}: {list_doc[i]}\\n\"\n",
    "    retriever_evaluation = aoai_client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": EVALUATION_PROMPT.format(documentation = document_str, question = my_question_answer[0] , response = my_question_answer[1])}],\n",
    "        temperature=0.2,\n",
    "        top_p = 1,\n",
    "        max_tokens=800).choices[0].message.content\n",
    "    \n",
    "    time.sleep(2)\n",
    "    list_evaluation.append(retriever_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_dataset[\"retriever_finds_document\"] = list_evaluation\n",
    "reduced_dataset[\"retriever_finds_document\"] = reduced_dataset[\"retriever_finds_document\"].apply(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_dataset.to_csv(\"retriever_evaluation.csv\", sep=\";\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
