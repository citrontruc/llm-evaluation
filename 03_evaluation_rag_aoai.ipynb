{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook's purpose is to evaluate the performance of RAG models for the Azure architecture.\n",
    "\n",
    "We have uploaded the wikipedia files to azure AI search and we want to evaluate our RAG models. In this notebook, we will test four configurations : \n",
    "- Azure AI Search (with simple search) + GPT-3.5\n",
    "- Azure AI Search (with hybrid search) + GPT-3.5\n",
    "- Azure AI Search (with hybrid search and a few more \"noise\" documents containing no answers) + GPT-3.5\n",
    "- Azure AI Search (with hybrid search and a few more \"noise\" documents containing no answers)  + Mistral Large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries & retrieve questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from openai import AzureOpenAI\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests.auth import AuthBase\n",
    "import time\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_ENDPOINT = os.environ.get(\"OPENAI_API_ENDPOINT\") \n",
    "OPENAI_API_VERSION = os.environ.get(\"OPENAI_API_VERSION\")\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "OPENAI_EMBEDDING_MODEL = os.environ.get(\"OPENAI_EMBEDDING_MODEL\")\n",
    "\n",
    "SEARCH_ENDPOINT = os.environ.get(\"SEARCH_ENDPOINT\")\n",
    "SEARCH_KEY =  os.environ.get(\"SEARCH_KEY\")\n",
    "SEARCH_INDEX = os.environ.get(\"SEARCH_INDEX\")\n",
    "\n",
    "MISTRAL_URL = os.environ.get(\"MISTRAL_URL\")\n",
    "MISTRAL_KEY = os.environ.get(\"MISTRAL_KEY\")\n",
    "\n",
    "aoai_client = AzureOpenAI(\n",
    "    azure_endpoint=OPENAI_API_ENDPOINT,\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    api_version=OPENAI_API_VERSION,\n",
    ")\n",
    "\n",
    "search_client = SearchClient(SEARCH_ENDPOINT, SEARCH_INDEX, AzureKeyCredential(SEARCH_KEY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change name of file depending on the name you chose for your question dataset.\n",
    "question_dataframe = pd.read_csv(\"question_benchmark.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of answers (GPT-35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We ask each of the questions to our RAG system. \n",
    "# Here, we evaluate the answers of a GPT-35 using simple search and hybrid search.\n",
    "\n",
    "list_classic_answer = []\n",
    "list_vector_answer = []\n",
    "\n",
    "for my_question in question_dataframe[\"question\"].values:\n",
    "    gpt_35_classic_answer = aoai_client.chat.completions.create(\n",
    "            model=\"gpt-35-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": my_question}],\n",
    "            temperature=0.2,\n",
    "            top_p = 1,\n",
    "            max_tokens=800,\n",
    "            extra_body={\n",
    "                \"data_sources\":[\n",
    "                    {\n",
    "                        \"type\" : \"azure_search\",\n",
    "                        \"parameters\": {\n",
    "                            \"endpoint\": SEARCH_ENDPOINT,\n",
    "                            \"index_name\": SEARCH_INDEX,\n",
    "                            \"role_information\": \"You are an AI assistant that helps people find information.\",\n",
    "                            \"query_type\": \"simple\",\n",
    "                            \"filter\": None,\n",
    "                            \"fields_mapping\": {},\n",
    "                            \"in_scope\": True,\n",
    "                            \"strictness\": 3,\n",
    "                            \"top_n_documents\": 5,\n",
    "                            \"authentication\": {\n",
    "                                \"type\": \"api_key\",\n",
    "                                \"key\": SEARCH_KEY\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ).choices[0].message.content\n",
    "\n",
    "    gpt_35_vector_answer = aoai_client.chat.completions.create(\n",
    "        model=\"gpt-35-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": my_question}],\n",
    "        temperature=0.2,\n",
    "        top_p = 1,\n",
    "        max_tokens=800,\n",
    "        extra_body={\n",
    "            \"data_sources\":[\n",
    "                {\n",
    "                    \"type\" : \"azure_search\",\n",
    "                    \"parameters\": {\n",
    "                        \"endpoint\": SEARCH_ENDPOINT,\n",
    "                        \"index_name\": SEARCH_INDEX,\n",
    "                        \"role_information\": \"You are an AI assistant that helps people find information.\",\n",
    "                        \"semantic_configuration\": \"default\",\n",
    "                        \"query_type\": \"vector_semantic_hybrid\",\n",
    "                        \"filter\": None,\n",
    "                        \"fields_mapping\": {},\n",
    "                        \"in_scope\": True,\n",
    "                        \"strictness\": 3,\n",
    "                        \"top_n_documents\": 5,\n",
    "                        \"authentication\": {\n",
    "                            \"type\": \"api_key\",\n",
    "                            \"key\": SEARCH_KEY\n",
    "                        },\n",
    "                        \"embedding_dependency\": {\n",
    "                            \"deployment_name\": OPENAI_EMBEDDING_MODEL,\n",
    "                            \"type\" : \"deployment_name\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ).choices[0].message.content\n",
    "    time.sleep(2)\n",
    "    list_classic_answer.append(gpt_35_classic_answer)\n",
    "    list_vector_answer.append(gpt_35_vector_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of answers (Mistral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for Mistral. We use a mistral large model deployed in Azure. Since there is no dedicated python library, we just do an API call using the requests library.\n",
    "# We create a method to generate an embedding from our question, a method to do research on our azure AI search an a prompt to link the two.\n",
    "\n",
    "class TokenAuth(AuthBase):\n",
    "    \"\"\"Implements a token authentication scheme.\"\"\"\n",
    "\n",
    "    def __init__(self, token):\n",
    "        self.token = token\n",
    "\n",
    "    def __call__(self, request):\n",
    "        \"\"\"Attach an API token to the Authorization header.\"\"\"\n",
    "        request.headers[\"Authorization\"] = f\"Bearer {self.token}\"\n",
    "        return request\n",
    "    \n",
    "MISTRAL_PROMPT = \"\"\"\n",
    "Your task is to answer a complicated factoid question given documents at your disposal.\n",
    "The questions demand a good understanding of the documents.\n",
    "Your factoid answer should be specific, concise informations retrieved from the documents.\n",
    "\n",
    "Documents:::\n",
    "{document}\n",
    "\n",
    "Question:::\n",
    "{question}\n",
    "\n",
    "Answer:::\"\"\"\n",
    "    \n",
    "# Method to embed a question using Azure OpenAI so that we can do hybrid search in Azure AI Search\n",
    "def generate_embeddings(user_question, aoai_client=aoai_client):\n",
    "    return aoai_client.embeddings.create(input = [user_question], model=OPENAI_EMBEDDING_MODEL).data[0].embedding\n",
    "\n",
    "def retrieve_doc(user_question, nb_doc=5, search_client=search_client):\n",
    "        \"\"\"\n",
    "        Function that takes as input a user question and uses Azure AI Search to return a list of documents and url that are on the topic of the user question.\n",
    "\n",
    "        input :\n",
    "            user_intent (str)\n",
    "\n",
    "        output :\n",
    "            doc_list (list)\n",
    "            url_list (list)\n",
    "        \"\"\"\n",
    "        embedded_query = generate_embeddings(user_question)\n",
    "        vector_query = VectorizedQuery(vector=embedded_query, k_nearest_neighbors=nb_doc, fields=\"contentVector\")\n",
    "        ai_search_results = search_client.search(\n",
    "            search_text = user_question,\n",
    "            search_fields=[\"content\"],\n",
    "            vector_queries = [vector_query],\n",
    "            select = [\"content\", \"title\", \"url\"],\n",
    "            query_type = \"semantic\",\n",
    "            semantic_configuration_name=\"default\",\n",
    "            top=nb_doc\n",
    "        )\n",
    "        \n",
    "        # Important documents and list of urls associated to the documents\n",
    "        url_list = []\n",
    "        doc_list = []\n",
    "        for my_result in ai_search_results:\n",
    "            doc_list.append(my_result[\"content\"])\n",
    "            url_list.append(my_result[\"url\"])\n",
    "        return list(set(doc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_mistral_answer = []\n",
    "\n",
    "for my_question in question_dataframe[\"question\"].values:\n",
    "    list_doc = retrieve_doc(my_question)\n",
    "    document_str = \"\"\n",
    "    for i in range(len(list_doc)):\n",
    "        document_str += f\"document number {i}: {list_doc[i]}\\n\"\n",
    "    response = requests.post(MISTRAL_URL + \"/v1/chat/completions\", auth=TokenAuth(MISTRAL_KEY), json = {\n",
    "        \"messages\":\n",
    "        [\n",
    "            { \n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You are an AI assistant that helps people find information.\"\n",
    "            },\n",
    "            {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": MISTRAL_PROMPT.format(document=document_str, question=my_question)\n",
    "            }\n",
    "        ],\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 1,\n",
    "        \"max_tokens\": 800\n",
    "    })\n",
    "    \n",
    "    time.sleep(2)\n",
    "    list_mistral_answer.append(json.loads(response.content.decode('utf-8'))[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt used for the evaluation.  \n",
    "\n",
    "EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "\n",
    "###The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evaluate:\n",
    "{response}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "[Is the response correct, accurate, and factual based on the reference answer?]\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "Score 4: The response is mostly correct, accurate, and factual.\n",
    "Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_answer(question, response, reference_answer, model=\"gpt-4\"):\n",
    "    return aoai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": EVALUATION_PROMPT.format(instruction = question, response = response, reference_answer = reference_answer)}],\n",
    "        temperature=0.2,\n",
    "        top_p = 1,\n",
    "        max_tokens=800).choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_questions = question_dataframe[\"question\"].values\n",
    "all_answer = question_dataframe[\"answer\"].values\n",
    "evaluation_classic = []\n",
    "evaluation_vector = []\n",
    "evaluation_mistral = []\n",
    "\n",
    "for i in range(len(question_dataframe)):\n",
    "    question_i = all_questions[i]\n",
    "    reference_answer_i = all_answer[i]\n",
    "    classic_answer_i = list_classic_answer[i]\n",
    "    vector_answer_i = list_vector_answer[i]\n",
    "    mistral_answer_i = list_mistral_answer[i]\n",
    "    evaluation_classic.append(evaluate_answer(question_i, classic_answer_i, reference_answer_i))\n",
    "    evaluation_vector.append(evaluate_answer(question_i, vector_answer_i, reference_answer_i))\n",
    "    evaluation_mistral.append(evaluate_answer(question_i, mistral_answer_i, reference_answer_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning answer from the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classic_critique = []\n",
    "classic_grade = []\n",
    "vector_critique = []\n",
    "vector_grade = []\n",
    "mistral_critique = []\n",
    "mistral_grade = []\n",
    "\n",
    "for i in tqdm(range(len(evaluation_classic))):\n",
    "    classic_critique.append(evaluation_classic[i].split(\"[RESULT]\")[0])\n",
    "    classic_grade.append(evaluation_classic[i].split(\"[RESULT]\")[1])\n",
    "    vector_critique.append(evaluation_vector[i].split(\"[RESULT]\")[0])\n",
    "    vector_grade.append(evaluation_vector[i].split(\"[RESULT]\")[1])\n",
    "    mistral_critique.append(evaluation_mistral[i].split(\"[RESULT]\")[0])\n",
    "    mistral_grade.append(evaluation_mistral[i].split(\"[RESULT]\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_dataframe[\"classic_anwer\"], question_dataframe[\"classic_critique\"], question_dataframe[\"classic_grade\"] = list_classic_answer, classic_critique, classic_grade\n",
    "question_dataframe[\"vector_anwer\"], question_dataframe[\"vector_critique\"], question_dataframe[\"vector_grade\"] = list_vector_answer, vector_critique, vector_grade\n",
    "question_dataframe[\"mistral_anwer\"], question_dataframe[\"mistral_critique\"], question_dataframe[\"mistral_grade\"] = list_mistral_answer, mistral_critique, mistral_grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_dataframe.to_csv(\"comparison.csv\", index=False, sep=\";\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
